--- a/llvm/tools/polly/lib/Makefile	(revision 156703)
+++ b/llvm/tools/polly/lib/Makefile	(working copy)
@@ -5,7 +5,7 @@
 #
 LEVEL :=..
 
-LIBRARYNAME=LLVMPolly
+LIBRARYNAME=libLLVMPolly
 LOADABLE_MODULE = 1
 
 include $(LEVEL)/Makefile.config
--- a/llvm/tools/polly/include/polly/ScopInfo.h	(revision 181135)
+++ b/llvm/tools/polly/include/polly/ScopInfo.h	(working copy)
@@ -92,9 +92,15 @@
     MayWrite
   };
 
+  enum RelationType {
+    RelationType_general = 0,
+    RelationType_polly = 1
+  };
+
 private:
-  isl_map *AccessRelation;
   enum AccessType Type;
+  unsigned ElemTypeSize;
 
   const Value *BaseAddr;
   std::string BaseName;
@@ -107,6 +113,10 @@
   /// Updated access relation read from JSCOP file.
   isl_map *newAccessRelation;
 
+  RelationType currentRelationType;
+  isl_map *GeneralAccessRelation;
+  isl_map *PollyAccessRelation;
+
 public:
   // @brief Create a memory access from an access in LLVM-IR.
   //
@@ -132,6 +142,14 @@
 
   isl_map *getAccessRelation() const;
 
+  void setCurrentRelationType(RelationType relationType);
+
+  RelationType getCurrentRelationType();
+
+  void setGeneralAccessRelation(isl_map *accessRelation);
+
+  void setPollyAccessRelation(isl_map *accessRelation);
+
   /// @brief Get an isl string representing this access function.
   std::string getAccessRelationStr() const;
 
@@ -139,6 +157,8 @@
 
   const std::string &getBaseName() const { return BaseName; }
 
+  unsigned getElemTypeSize() const { return ElemTypeSize; }
+
   const Instruction *getAccessInstruction() const { return Inst; }
 
   /// @brief Get the new access function imported from JSCOP file
--- a/llvm/tools/polly/lib/Analysis/Dependences.cpp	(revision 181135)
+++ b/llvm/tools/polly/lib/Analysis/Dependences.cpp	(working copy)
@@ -30,6 +30,10 @@
 #include "llvm/Support/Debug.h"
 #include "llvm/Support/CommandLine.h"
 
+#define CLOOG_INT_GMP 1
+#include "cloog/cloog.h"
+#include "cloog/isl/cloog.h"
+
 #include <isl/aff.h>
 #include <isl/flow.h>
 #include <isl/map.h>
@@ -242,6 +246,13 @@
   return IsParallel;
 }
 
+bool Dependences::isParallelFor(const clast_for *f) {
+  isl_set *Domain = isl_set_from_cloog_domain(f->domain);
+  assert(Domain && "Cannot access domain of loop");
+
+  return isParallelDimension(isl_set_copy(Domain), isl_set_n_dim(Domain));
+}
+
 void Dependences::printScop(raw_ostream &OS) const {
   std::string RAWString, WARString, WAWString;
 
--- a/llvm/tools/polly/include/polly/Dependences.h	(revision 181135)
+++ b/llvm/tools/polly/include/polly/Dependences.h	(working copy)
@@ -84,6 +84,17 @@
   bool isParallelDimension(__isl_take isl_set *LoopDomain,
                            unsigned ParallelDimension);
 
+  /// @brief Check if a loop is parallel
+  ///
+  /// Detect if a clast_for loop can be executed in parallel.
+  ///
+  /// @param f The clast for loop to check.
+  ///
+  /// @return bool Returns true if the incoming clast_for statement can
+  ///              execute in parallel.
+  bool isParallelFor(const clast_for *For);
+
+
   /// @brief Get the dependences in this Scop.
   ///
   /// @param Kinds This integer defines the different kinds of dependences
--- a/llvm/tools/polly/lib/Analysis/ScopInfo.cpp	(revision 181135)
+++ b/llvm/tools/polly/lib/Analysis/ScopInfo.cpp	(working copy)
@@ -225,7 +225,10 @@
 //===----------------------------------------------------------------------===//
 
 MemoryAccess::~MemoryAccess() {
-  isl_map_free(AccessRelation);
+  if (PollyAccessRelation)
+    isl_map_free(PollyAccessRelation);
+  if (GeneralAccessRelation)
+    isl_map_free(GeneralAccessRelation);
   isl_map_free(newAccessRelation);
 }
 
@@ -254,11 +257,43 @@
 }
 
 isl_map *MemoryAccess::getAccessRelation() const {
-  return isl_map_copy(AccessRelation);
+  return (CurrentRelationType == RelationType_polly) ?
+    isl_map_copy(PollyAccessRelation) : isl_map_copy(GeneralAccessRelation);
 }
 
+void  MemoryAccess::setCurrentRelationType(MemoryAccess::RelationType relationType)
+{
+  if (relationType == RelationType_polly)
+    assert(PollyAccessRelation &&
+      "PollyAccessRelation was set before setCurrentRelationType");
+  else
+    assert(GeneralAccessRelation && "GeneralAccessRelation was set before setCurrentRelationType");
+  CurrentRelationType = relationType;
+}
+
+MemoryAccess::RelationType MemoryAccess::getCurrentRelationType()
+{
+  return CurrentRelationType;
+}
+
+void MemoryAccess::setGeneralAccessRelation(isl_map *accessRelation)
+{
+  if (GeneralAccessRelation)
+    isl_map_free(GeneralAccessRelation);
+  GeneralAccessRelation = isl_map_copy(accessRelation);
+}
+
+void MemoryAccess::setPollyAccessRelation(isl_map *accessRelation)
+{
+  if (PollyAccessRelation)
+    isl_map_free(PollyAccessRelation);
+  PollyAccessRelation = isl_map_copy(accessRelation);
+}
+
 std::string MemoryAccess::getAccessRelationStr() const {
-  return stringFromIslObj(AccessRelation);
+  isl_map *relation = (CurrentRelationType == RelationType_polly) ?
+    PollyAccessRelation:GeneralAccessRelation;
+  return stringFromIslObj(relation);
 }
 
 isl_map *MemoryAccess::getNewAccessRelation() const {
@@ -277,7 +312,8 @@
 
 MemoryAccess::MemoryAccess(const IRAccess &Access, const Instruction *AccInst,
                            ScopStmt *Statement)
-    : Inst(AccInst) {
+    : Inst(AccInst), GeneralAccessRelation(NULL),
+      CurrentRelationType(RelationType_polly) {
   newAccessRelation = NULL;
   Type = Access.isRead() ? Read : Write;
   statement = Statement;
@@ -287,7 +323,7 @@
 
   if (!Access.isAffine()) {
     Type = (Type == Read) ? Read : MayWrite;
-    AccessRelation = isl_map_from_basic_map(createBasicAccessMap(Statement));
+    PollyAccessRelation = isl_map_from_basic_map(createBasicAccessMap(Statement));
     return;
   }
 
@@ -303,33 +339,40 @@
   isl_int v;
   isl_int_init(v);
   isl_int_set_si(v, Access.getElemSizeInBytes());
+  ElemTypeSize = Access.getElemSizeInBytes();
   Affine = isl_pw_aff_scale_down(Affine, v);
   isl_int_clear(v);
 
-  AccessRelation = isl_map_from_pw_aff(Affine);
+  PollyAccessRelation = isl_map_from_pw_aff(Affine);
   isl_space *Space = Statement->getDomainSpace();
-  AccessRelation = isl_map_set_tuple_id(
+  PollyAccessRelation = isl_map_set_tuple_id(
       PollyAccessRelation, isl_dim_in, isl_space_get_tuple_id(Space, isl_dim_set));
   isl_space_free(Space);
-  AccessRelation = isl_map_set_tuple_name(AccessRelation, isl_dim_out,
+  PollyAccessRelation = isl_map_set_tuple_name(
       PollyAccessRelation, isl_dim_out, getBaseName().c_str());
 }
 
 void MemoryAccess::realignParams() {
+  assert(CurrentRelationType == RelationType_polly &&
+    "Polly relation type is expected");
   isl_space *ParamSpace = statement->getParent()->getParamSpace();
-  AccessRelation = isl_map_align_params(AccessRelation, ParamSpace);
+  PollyAccessRelation = isl_map_align_params(PollyAccessRelation, ParamSpace);
+  if (GeneralAccessRelation)
+    GeneralAccessRelation = isl_map_align_params(GeneralAccessRelation,
+                                                 ParamSpace);
 }
 
-MemoryAccess::MemoryAccess(const Value *BaseAddress, ScopStmt *Statement) {
+MemoryAccess::MemoryAccess(const Value *BaseAddress, ScopStmt *Statement)
+    : GeneralAccessRelation(NULL), CurrentRelationType(RelationType_polly) {
   newAccessRelation = NULL;
   BaseAddr = BaseAddress;
   Type = Read;
   statement = Statement;
 
   isl_basic_map *BasicAccessMap = createBasicAccessMap(Statement);
-  AccessRelation = isl_map_from_basic_map(BasicAccessMap);
+  PollyAccessRelation = isl_map_from_basic_map(BasicAccessMap);
   isl_space *ParamSpace = Statement->getParent()->getParamSpace();
-  AccessRelation = isl_map_align_params(AccessRelation, ParamSpace);
+  PollyAccessRelation = isl_map_align_params(PollyAccessRelation, ParamSpace);
 }
 
 void MemoryAccess::print(raw_ostream &OS) const {
@@ -352,6 +395,7 @@
 //     : i0 = o0, i1 = o1, ..., i(X-1) = o(X-1), iX < oX
 //
 static isl_map *getEqualAndLarger(isl_space *setDomain) {
+   
   isl_space *Space = isl_space_map_from_set(setDomain);
   isl_map *Map = isl_map_universe(isl_space_copy(Space));
   isl_local_space *MapLocalSpace = isl_local_space_from_space(Space);
@@ -421,10 +465,14 @@
 }
 
 bool MemoryAccess::isStrideZero(const isl_map *Schedule) const {
+  assert(CurrentRelationType == RelationType_polly &&
+    "Polly relation type is expected");
   return isStrideX(Schedule, 0);
 }
 
 bool MemoryAccess::isStrideOne(const isl_map *Schedule) const {
+  assert(CurrentRelationType == RelationType_polly &&
+    "Polly relation type is expected");
   return isStrideX(Schedule, 1);
 }
 
Index: ScopDetection.cpp
===================================================================
--- a/llvm/tools/polly/lib/Analysis/ScopDetection.cpp	(revision 181135)
+++ b/llvm/tools/polly/lib/Analysis/ScopDetection.cpp	(working copy)
@@ -74,7 +74,7 @@
              cl::Hidden, cl::value_desc("The function name to detect scops in"),
              cl::ValueRequired, cl::init(""));
 
-static cl::opt<bool>
+cl::opt<bool>
 IgnoreAliasing("polly-ignore-aliasing",
                cl::desc("Ignore possible aliasing of the array bases"),
                cl::Hidden, cl::init(false));
--- a/llvm/tools/polly/lib/CodeGen/CodeGeneration.cpp	(revision 181135)
+++ b/llvm/tools/polly/lib/CodeGen/CodeGeneration.cpp	(working copy)
@@ -40,6 +40,7 @@
 #include "llvm/ADT/SetVector.h"
 #include "llvm/ADT/PostOrderIterator.h"
 #include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Analysis/Verifier.h"
 #include "llvm/Analysis/ScalarEvolutionExpander.h"
 #include "llvm/Support/CommandLine.h"
 #include "llvm/Support/Debug.h"
@@ -54,9 +55,11 @@
 
 #include <vector>
 #include <utility>
+#include <string>
 
 using namespace polly;
 using namespace llvm;
+using namespace std;
 
 struct isl_set;
 
@@ -77,6 +80,20 @@
     cl::Hidden, cl::init(""));
 #endif /* GPU_CODEGEN */
 
+cl::opt<bool>
+CUDA("enable-polly-CUDA",
+     cl::desc("Enable polly CUDA code generation"), cl::Hidden,
+     cl::value_desc("CUDA code generation enabled if true"),
+     cl::init(false));
+
+// CudaFunctions are extern functions defining thread position in grid.
+// Will be filled in CodeGeneration::addCUDADefinitions.
+static vector<const char *> CudaFunctions;
+static DenseMap<const char *, const char *> CudaIntrinsics;
+
+// Dimensions contains the names of dimensions. Default are x, y, z.
+std::vector<string> dimensions;
+
 typedef DenseMap<const char *, Value *> CharMapT;
 
 /// Class to generate LLVM-IR that calculates the value of a clast_expr.
@@ -166,6 +183,59 @@
   llvm_unreachable("Unknown clast binary expression type");
 }
 
+static Value *createLoopForCUDA(IRBuilder<> *Builder, Value *LB, Value *UB,
+  Value *ThreadLB, Value *ThreadUB, Value *ThreadStride,
+  const char * dimension, Pass *P, BasicBlock **AfterBlock) {
+  Function *F = Builder->GetInsertBlock()->getParent();
+  LLVMContext &Context = F->getContext();
+
+  BasicBlock *PreheaderBB = Builder->GetInsertBlock();
+  BasicBlock *HeaderBB = BasicBlock::Create(Context, (string)"CUDA.LoopHeader." + dimension, F);
+  BasicBlock *BodyBB = BasicBlock::Create(Context, (string)"CUDA.LoopBody." + dimension, F);
+
+  BasicBlock *AfterBB = SplitBlock(PreheaderBB, Builder->GetInsertPoint()++, P);
+  AfterBB->setName((string)"CUDA.AfterLoop." + dimension);
+
+  PreheaderBB->getTerminator()->setSuccessor(0, HeaderBB);
+  DominatorTree &DT = P->getAnalysis<DominatorTree>();
+  DT.addNewBlock(HeaderBB, PreheaderBB);
+  Builder->SetInsertPoint(HeaderBB);
+
+  // Use the type of upper and lower bound.
+  assert(LB->getType() == UB->getType()
+    && "Different types for upper and lower bound.");
+
+  IntegerType *LoopIVType = dyn_cast<IntegerType>(UB->getType());
+  assert(LoopIVType && "UB is not integer?");
+
+  // IV
+  PHINode *IV = Builder->CreatePHI(LoopIVType, 2, (string)"CUDA.loopiv." + dimension);
+  IV->addIncoming(ThreadLB, PreheaderBB);
+
+  // IV increment.
+  Value *StrideValue = ThreadStride;
+  Value *IncrementedIV = Builder->CreateAdd(IV, StrideValue, (string)"CUDA.next_loopiv." + dimension);
+
+  // Exit condition.
+  // Maybe not executed at all.
+  // next iteration performed if loop condition is true:
+  // InductionVariable <= ThreadUpperBound && InductionVariable <= LoopUpperBound
+  Value *ExitCond = Builder->CreateICmpSLE(IV, ThreadUB, (string)"isInThreadBounds." + dimension); 
+
+  Builder->CreateCondBr(ExitCond, BodyBB, AfterBB);
+  DT.addNewBlock(BodyBB, HeaderBB);
+
+  Builder->SetInsertPoint(BodyBB);
+  Builder->CreateBr(HeaderBB);
+  IV->addIncoming(IncrementedIV, BodyBB);
+  DT.changeImmediateDominator(AfterBB, HeaderBB);
+
+  Builder->SetInsertPoint(BodyBB->begin());
+  *AfterBlock = AfterBB;
+
+  return IV;
+}
+
 Value *ClastExpCodeGen::codegen(const clast_reduction *r, Type *Ty) {
   assert((r->type == clast_red_min || r->type == clast_red_max ||
           r->type == clast_red_sum) && "Clast reduction type not supported");
@@ -216,6 +286,24 @@
 public:
   const std::vector<std::string> &getParallelLoops();
 
+  // Each thread has it's own position in Grid
+  // That position is computed in runtime for each dimension of grid
+  vector<Value*> BlockPositionInGrid;
+  vector<Value*> ThreadPositionInBlock;
+
+  // For each dimension of grid computes it's size (count of threads)
+  // GridSize contains respectively Value*
+  vector<Value*> GridSize;
+
+  // For each dimension of block it's size obtained by call to one of the CUDA Functions
+  // BlockSize contains respectively Value*
+  vector<Value*> BlockSize;
+
+  int goodNestedParallelLoopsCount;
+
+  // Maximal count of good nested parallel loops, which can be parallelized
+  int MaxDimensionsCount;
+
 private:
   // The Scop we code generate.
   Scop *S;
@@ -317,6 +405,228 @@
                   unsigned &LoopDepth, unsigned &NonPLoopDepth);
 #endif /* GPU_CODEGEN */
 
+  void createCUDAGridParamsAndPosInGridBlocks() {
+    Module *M = Builder.GetInsertBlock()->getParent()->getParent();
+    vector<Value *> GridParameters;
+
+    // GridParams BasicBlock - load Grid parameters by calling CUDA functions
+    BasicBlock *GridParamsBB = Builder.GetInsertBlock();
+    GridParamsBB->setName("CUDA.getGridParams");
+
+    // PosInGrid BasicBlock - compute thread positin in grid
+    BasicBlock *PosInGridBB = SplitBlock(GridParamsBB,
+                                         GridParamsBB->getTerminator(), P);
+    PosInGridBB->setName("CUDA.getPositionInGrid");
+
+    // Compute needed values separately for each dimension.
+    for (int dimension = 0; dimension < goodNestedParallelLoopsCount;
+        dimension++) {
+      Builder.SetInsertPoint(GridParamsBB->getTerminator());
+
+      // Call CUDA functions and store values in vector GridParameters.
+      for (int GridParameter = 0; GridParameter < 4; GridParameter ++) {
+        GridParameters.push_back(Builder.CreateCall(
+          M->getFunction(CudaFunctions[dimension * 4 + GridParameter]),
+            CudaIntrinsics[CudaFunctions[dimension * 4 + GridParameter]]));
+      }
+
+      Builder.SetInsertPoint(PosInGridBB->getTerminator());
+
+      // Grid Parameters for current dimension
+      Value *threadId = GridParameters[dimension * 4 + 0];
+      Value *blockId  = GridParameters[dimension * 4 + 1];
+      Value *blockDim = GridParameters[dimension * 4 + 2];
+      Value *gridDim  = GridParameters[dimension * 4 + 3];
+
+      // Absolute position of block's first thread (position of block)
+      // blockId.x * blockDim.x - "position of block's thread 0 for dimension x"
+      Value* Position = Builder.CreateMul(blockId, blockDim,
+                                          string("PositionOfBlockInGrid.") +
+                                          dimensions[dimension]);
+
+      // GridDim.x * blockDim.x - size of grid in threads for dimension x
+      Value * Size = Builder.CreateMul(gridDim, blockDim,
+                                       string("GridSize.") +
+                                       dimensions[dimension]);
+
+      // Store values.
+      BlockPositionInGrid.push_back(Position);
+      ThreadPositionInBlock.push_back(threadId);
+      GridSize.push_back(Size);
+      BlockSize.push_back(blockDim);
+    }
+
+    BasicBlock *LoopPreheader = SplitBlock(PosInGridBB,PosInGridBB->getTerminator(),P);
+    Builder.SetInsertPoint(LoopPreheader->getTerminator());
+  }
+  
+  void codegenForCUDA(const clast_for *f) {
+    // At this point GridParamsBB and PosInGridBB BasicBlocks are already created.
+    // The needed Value*-es are stored in positionInGrid, GridSize, BlockSize for each thread.
+    int dimension = goodNestedParallelLoopsCount - parallelLoops.size();
+    const char * dimensionName = dimensions[dimension].c_str();
+
+    // In CountBoundsBB BasicBlock for each dimension compute:
+    //   CountOfIterations
+    //   ThreadUpperBound
+    //   ThreadLowerBound
+    //   Stride
+    // These values are different between threads in runtime.
+    BasicBlock * CountBoundsBB = Builder.GetInsertBlock();
+    CountBoundsBB->setName(string("CUDA.CountBounds.") + dimensionName);
+
+    IntegerType * IntType = Type::getInt32Ty(Builder.getContext());
+
+    // Lower and Upper Bounds of Loop
+    Value *lowerBound = ExpGen.codegen(f->LB,IntType);
+    Value *upperBound = ExpGen.codegen(f->UB,IntType);
+
+    // Stride of loop
+    assert(polly::APInt_from_MPZ(f->stride) != 0 &&
+      "Loop stride can't be zero");
+    assert(polly::APInt_from_MPZ(f->stride).getSExtValue() > 0 &&
+      "TODO: support of negative stride");
+
+    Value *LoopStride = ConstantInt::get(IntType,
+      polly::APInt_from_MPZ(f->stride).zext(IntType->getBitWidth()));
+
+    // The number of loop's iterations.
+    // ((UpperBound - LowerBound) / stride + 1)
+    // The number of iterations minus one = (UpperBound - LowerBound) / stride
+    Value *UpperMinusLower = Builder.CreateSub(
+      upperBound, lowerBound,
+      string("UB.minus.LB.") + dimensionName);
+    Value *NumOfIterationsMinusOne = Builder.CreateSDiv(
+      UpperMinusLower, LoopStride,
+      string("NumOfIterationsMinusOne.") + dimensionName);
+
+    // Compute number of Iterations per thread.
+    // ((NumberOfIterations - 1) / GridSize) + 1)
+    // ( NumOfIterationsMinusOne / GridSize + 1)
+    Value *One = ConstantInt::get(lowerBound->getType(), 1);
+    Value *IterationsPerThreadMinusOne = Builder.CreateSDiv(
+      NumOfIterationsMinusOne, GridSize[dimension],
+      string("IterationsPerThreadMinusOne.") + dimensionName);
+    Value *IterationsPerThread = Builder.CreateAdd(
+      IterationsPerThreadMinusOne, One,
+      string("IterationsPerThread.") + dimensionName);
+
+    // Compute Thread's Upper and Lower Bounds and Stride
+    // ThreadLowerBound = LoopStride * (IterationsPerThread * BlockPosition + ThreadPositionInBlock)
+    // ThreadUpperBound = ThreadLowerBound + ThreadStride * (IterationsPerThread - 1)
+    // Stride = BlockSize (to increase probability of coalescing transactions to memory)
+    Value *BlockLowerBound = Builder.CreateMul(
+      BlockPositionInGrid[dimension], IterationsPerThread,
+      string("BlockLowerBound.") + dimensionName);
+    Value *BlockLBAddThreadPosInBlock = Builder.CreateAdd(
+      BlockLowerBound, ThreadPositionInBlock[dimension],
+      string("BlockLB.Add.ThreadPosInBlock.") + dimensionName);
+    Value *ThreadLowerBound = Builder.CreateMul(
+      BlockLBAddThreadPosInBlock, LoopStride,
+      string("ThreadLowerBound.") + dimensionName);
+    Value *ThreadStride = Builder.CreateMul(
+      LoopStride, BlockSize[dimension],
+      string("ThreadStride.") + dimensionName);
+    Value *StrideMulIterPerThreadMinusOne = Builder.CreateMul(
+      IterationsPerThreadMinusOne, ThreadStride,
+      string("ThreadStride.Mul.IterPerThreadMinusOne.") + dimensionName);
+    Value *ThreadUpperBound = Builder.CreateAdd(
+      ThreadLowerBound, StrideMulIterPerThreadMinusOne,
+      string("ThreadUpperBound.") + dimensionName);
+
+    // Generate code for loop with computed bounds and stride
+    // CountBoundsBB BasicBlock is a preheader of that loop
+    BasicBlock *AfterBB;
+    Value *IV = createLoopForCUDA(&Builder, lowerBound, upperBound,
+      ThreadLowerBound, ThreadUpperBound, ThreadStride, dimensionName,
+      P, &AfterBB);
+
+    ClastVars[f->iterator] = IV;
+    if (f->body) codegen(f->body);
+
+    // Loop is finished, so remove its iv from the live symbols.
+    ClastVars.erase(f->iterator);
+    AfterBB->moveAfter(Builder.GetInsertBlock());
+
+    // Make block for truncation of threadUpperBound.
+    BasicBlock *truncateThreadUB = SplitBlock(CountBoundsBB, CountBoundsBB->getTerminator(), P);
+    truncateThreadUB->setName(string("CUDA.truncateThreadUB.") + dimensionName);
+    Builder.SetInsertPoint(truncateThreadUB->getTerminator());
+
+    // if(threadUpperBound > loopUpperBound) threadUpperBound = loopUpperBound;
+    Value *isThreadUBgtLoopUB = Builder.CreateICmpSGT(
+      ThreadUpperBound, upperBound, string("isThreadUBgtLoopUB.") + dimensionName);
+    ThreadUpperBound = Builder.CreateSelect(
+      isThreadUBgtLoopUB, upperBound, ThreadUpperBound,
+      string("truncatedThreadUB.") + dimensionName);
+
+    // Get terminator of CountBoundsBB.
+    TerminatorInst * terminator = CountBoundsBB->getTerminator();
+    Builder.SetInsertPoint(CountBoundsBB);
+    // if(threadLowerBound > loopUpperBound) then no execute body et all
+    Value *isThreadLBgtLoopUB = Builder.CreateICmpSGT(
+      ThreadLowerBound, upperBound, string("isThreadLBgtLoopUB.") + dimensionName);
+    Builder.CreateCondBr(isThreadLBgtLoopUB, AfterBB, truncateThreadUB);
+
+    // Erase the old terminator.
+    terminator->eraseFromParent();
+
+    Builder.SetInsertPoint(AfterBB->begin());
+  }
+
+  // Returns true if:
+  // = the list does not contain nested loops;
+  // = the list contains only one nested loop and does not contain assignments or
+  //   user_stmt-s outside this nested list.
+  // Also returns the nested loop pointer to nested_for, if it exists; or NULL otherwise.
+  bool isaGoodListOfStatements(const clast_stmt * stmt,
+    const clast_for * &nested_for, bool & user_or_assignment)
+  {
+    if (!stmt) return true;
+    bool good = true;
+
+    if (CLAST_STMT_IS_A(stmt, stmt_user) || CLAST_STMT_IS_A(stmt, stmt_ass)) {
+      if (nested_for) return false; // there is already anoter loop
+      else user_or_assignment = true;
+    }
+
+    if (CLAST_STMT_IS_A(stmt, stmt_guard))
+      good = isaGoodListOfStatements(
+        ((const clast_guard *)stmt)->then, nested_for, user_or_assignment);
+
+    if (CLAST_STMT_IS_A(stmt, stmt_block))
+      good = isaGoodListOfStatements(
+        ((const clast_block *)stmt)->body, nested_for, user_or_assignment);
+
+    if (CLAST_STMT_IS_A(stmt, stmt_for)) {
+      if (nested_for || user_or_assignment)
+        return false; // there is already a loop or a user_stmt or an assignment
+      else
+        nested_for = (const clast_for *)stmt; // found a loop
+    }
+
+    return good && isaGoodListOfStatements( stmt->next, nested_for, user_or_assignment);
+  }
+
+  int GoodNestedParallelLoops(const clast_stmt * stmt)
+  {
+    int goodLoops = 0;
+    while (goodLoops < 3) {
+      const clast_for *nested_for = NULL;
+      bool user_or_assignment = false;
+      if (isaGoodListOfStatements(stmt, nested_for, user_or_assignment)) {
+        if (nested_for) // if there is a loop
+          if (isParallelFor(nested_for)) { // and it is parallel
+            goodLoops++; // then increment the good loops counter
+            stmt = nested_for->body; // and check the body of this loop
+          } else break; // if there is a non-parallel loop
+        else break; // if no more loops
+      } else break;
+    }
+
+    return goodLoops;
+  }
+
   /// @brief Check if a loop is parallel
   ///
   /// Detect if a clast_for loop can be executed in parallel.
@@ -870,6 +1180,14 @@
   }
 #endif
 
+  if (CUDA && isParallelFor(f)) {
+    if ((int)parallelLoops.size() < goodNestedParallelLoopsCount) {
+      parallelLoops.push_back(f->iterator);
+      codegenForCUDA(f);
+      return;
+    }
+  }
+
   codegenForSequential(f);
 }
 
@@ -893,7 +1211,7 @@
   LLVMContext &Context = F->getContext();
 
   BasicBlock *CondBB =
-      SplitBlock(Builder.GetInsertBlock(), Builder.GetInsertPoint(), P);
+        SplitBlock(Builder.GetInsertBlock(), Builder.GetInsertPoint(), P);
   CondBB->setName("polly.cond");
   BasicBlock *MergeBB = SplitBlock(CondBB, CondBB->begin(), P);
   MergeBB->setName("polly.merge");
@@ -983,6 +1301,52 @@
 
   CodeGeneration() : ScopPass(ID) {}
 
+  // Adding prototypes required if OpenMP is enabled.                    
+  // For each dimension defines four functions, which returns parameters 
+  //  threadId                                                           
+  //      blockId                                                            
+  //      BlockDim                                                           
+  //      gridDim                                                            
+  // for a dimension                                                     
+  void addCUDADeclarations(Module *M) {
+    IRBuilder<> Builder(M->getContext());
+    LLVMContext &Context = Builder.getContext();
+    IntegerType *intType = Type::getInt32Ty(Context);
+
+    if (!M->getFunction("llvm.nvvm.read.ptx.sreg.tid.x")) {
+      //  Define all dimensions, that can be used while code generation.
+      dimensions.push_back("x");
+      dimensions.push_back("y");
+      dimensions.push_back("z");
+
+      // Define parameters of dimensions.
+      vector<string> parameters;
+      parameters.push_back("tid");
+      parameters.push_back("ctaid");
+      parameters.push_back("ntid");
+      parameters.push_back("nctaid");
+
+      string prefix1("llvm.nvvm.read.ptx.sreg.");
+      string prefix2(".");
+      string prefix3(".");
+
+      for (unsigned int i = 0; i < dimensions.size(); i++)
+        for (unsigned int j = 0; j < parameters.size(); j++) {
+          CudaFunctions.push_back((new string(prefix1 + parameters[j] + 
+            prefix2 + dimensions[i]))->c_str());
+          CudaIntrinsics[CudaFunctions.back()] = (new string(parameters[j] +
+            prefix3 + dimensions[i]))->c_str();
+        }
+
+      for (unsigned int i = 0; i < CudaFunctions.size(); i++) {
+        FunctionType *FT = FunctionType::get(
+          intType, std::vector<Type*>(), false);
+        Function::Create(
+          FT, Function::ExternalLinkage, (CudaFunctions)[i], M);
+      }
+    }
+  }
+
   bool runOnScop(Scop &S) {
     ParallelLoops.clear();
 
@@ -991,6 +1355,9 @@
 
     simplifyRegion(&S, this);
 
+    Module *M = region->getEntry()->getParent()->getParent();
+    if (CUDA) addCUDADeclarations(M);
+
     BasicBlock *StartBlock = executeScopConditionally(S, this);
 
     IRBuilder<> Builder(StartBlock->begin());
@@ -1002,6 +1369,10 @@
     ParallelLoops.insert(ParallelLoops.begin(),
                          CodeGen.getParallelLoops().begin(),
                          CodeGen.getParallelLoops().end());
+
+    assert(!verifyFunction(*(region->getEntry()->getParent())) && 
+                           "Verification failed for the generated function");
+
     return true;
   }
 
